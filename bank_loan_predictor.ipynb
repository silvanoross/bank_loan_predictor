{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bank_loan_predictor Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns are age, balance, day, duration, campaign, pdays, previous.\n",
      "Categorical columns are job, marital, education, default, housing, loan, contact, month, poutcome.\n",
      "Training data has 40689 rows.\n",
      "Test data has 4522 rows.\n",
      "Featurized training data has 40689 rows and 51 columns.\n",
      "Featurized test data has 4522 rows and 51 columns.\n",
      "Precision = 65% and recall = 35% on the training data.\n",
      "Precision = 63% and recall = 34% on the validation data.\n"
     ]
    }
   ],
   "source": [
    "# frontload all of the library imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# set global state variable\n",
    "SEED = 42\n",
    "\n",
    "# read the csv into a dataframe\n",
    "bank = pd.read_csv('./bank-full.csv', delimiter = ';')\n",
    "\n",
    "# separate numeric, categorical and target columns\n",
    "num_cols = bank.select_dtypes(['integer', 'float']).columns\n",
    "cat_cols = bank.select_dtypes(['object']).drop(columns = \"y\").columns\n",
    "\n",
    "# print column types\n",
    "print(\"Numeric columns are {}.\".format(\", \".join(num_cols)))\n",
    "print(\"Categorical columns are {}.\".format(\", \".join(cat_cols)))\n",
    "\n",
    "# split the data into training and testing data as 90-10 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(bank.drop(columns = \"y\"), bank[\"y\"], \n",
    "                                                    test_size = 0.10, random_state = 42)\n",
    "# reset index of training and testing data\n",
    "X_train = X_train.reset_index(drop = True)\n",
    "X_test = X_test.reset_index(drop = True)\n",
    "\n",
    "# show the shape of the training and testing data\n",
    "print(\"Training data has {} rows.\".format(X_train.shape[0]))\n",
    "print(\"Test data has {} rows.\".format(X_test.shape[0]))\n",
    "\n",
    "#------------------------------------------------------------------------------------------#\n",
    "\n",
    "# One hot encode the categorical data\n",
    "onehotter = OneHotEncoder(sparse_output = False)\n",
    "onehotter.fit(X_train[cat_cols])\n",
    "onehot_cols = onehotter.get_feature_names_out(cat_cols)\n",
    "X_train_onehot = pd.DataFrame(onehotter.transform(X_train[cat_cols]), columns = onehot_cols)\n",
    "X_test_onehot = pd.DataFrame(onehotter.transform(X_test[cat_cols]), columns = onehot_cols)\n",
    "\n",
    "# Train normalizer using z-score normalization\n",
    "znormalizer = StandardScaler()\n",
    "znormalizer.fit(X_train[num_cols])\n",
    "\n",
    "# transform numeric columns \n",
    "X_train_norm = pd.DataFrame(znormalizer.transform(X_train[num_cols]), columns = num_cols)\n",
    "X_test_norm = pd.DataFrame(znormalizer.transform(X_test[num_cols]), columns = num_cols)\n",
    "\n",
    "# build out featurized dataframes\n",
    "X_train_featurized = X_train_onehot # add one-hot-encoded columns\n",
    "X_test_featurized = X_test_onehot   # add one-hot-encoded columns\n",
    "X_train_featurized[num_cols] = X_train_norm # add numeric columns\n",
    "X_test_featurized[num_cols] = X_test_norm   # add numeric columns\n",
    "\n",
    "# delete unneeded dataframes from memory\n",
    "del X_train_norm, X_test_norm, X_train_onehot, X_test_onehot\n",
    "\n",
    "# show infor about featurized dataframes to compare to non-featurized\n",
    "print(\"Featurized training data has {} rows and {} columns.\".format(*X_train_featurized.shape))\n",
    "print(\"Featurized test data has {} rows and {} columns.\".format(*X_test_featurized.shape))\n",
    "\n",
    "#------------------------------------------------------------------------------------------#\n",
    "\n",
    "# instantiate logistic regressor\n",
    "logit = LogisticRegression(max_iter = 5000, solver = 'lbfgs', random_state=SEED)\n",
    "logit.fit(X_train_featurized, y_train)\n",
    "\n",
    "y_hat_train = logit.predict(X_train_featurized) # predict training data\n",
    "y_hat_test = logit.predict(X_test_featurized) # predict testing data\n",
    "\n",
    "# Show precision and recall for both training and testing predictions\n",
    "precision_train = precision_score(y_train, y_hat_train, pos_label = 'yes') * 100\n",
    "precision_test = precision_score(y_test, y_hat_test, pos_label = 'yes') * 100\n",
    "\n",
    "recall_train = recall_score(y_train, y_hat_train, pos_label = 'yes') * 100\n",
    "recall_test = recall_score(y_test, y_hat_test, pos_label = 'yes') * 100\n",
    "\n",
    "print(\"Precision = {:.0f}% and recall = {:.0f}% on the training data.\".format(precision_train, recall_train))\n",
    "print(\"Precision = {:.0f}% and recall = {:.0f}% on the validation data.\".format(precision_test, recall_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above trains a model on data that contains both categorical and numeric features. I normalize the numeric features and one-hot-encode the categorical features as part of pre-processing. In the above code I do this \"manually\", however as shown [here](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer.html) we can compose data transformations and ML steps to create a **single multi-step pipeline**. The pipeline object (conviniently called `pipeline` in the docs) has a `fit` and `predict` method:\n",
    "- By calling `fit`, the raw input data is first transformed into the featurized data, and then passed to the ML algorithm to train a model.\n",
    "- By calling `predict`, the raw input data is first transformed into the featurized data (just like `fit`), and and then used to get predictions (using the model trained when we called `fit`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create local data folder to store\n",
    "data_folder = \"data\"\n",
    "os.makedirs(data_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will be a json file containing the new data we will use to predict bank loan approval later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/new_data.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/new_data.json\n",
    "{\"age\": {\"0\": 40, \"1\": 47},\n",
    " \"balance\": {\"0\": 580, \"1\": 3644},\n",
    " \"campaign\": {\"0\": 1, \"1\": 2},\n",
    " \"contact\": {\"0\": \"unknown\", \"1\": \"unknown\"},\n",
    " \"day\": {\"0\": 16, \"1\": 9},\n",
    " \"default\": {\"0\": \"no\", \"1\": \"no\"},\n",
    " \"duration\": {\"0\": 192, \"1\": 83},\n",
    " \"education\": {\"0\": \"secondary\", \"1\": \"secondary\"},\n",
    " \"housing\": {\"0\": \"yes\", \"1\": \"no\"},\n",
    " \"job\": {\"0\": \"blue-collar\", \"1\": \"services\"},\n",
    " \"loan\": {\"0\": \"no\", \"1\": \"no\"},\n",
    " \"marital\": {\"0\": \"married\", \"1\": \"single\"},\n",
    " \"month\": {\"0\": \"may\", \"1\": \"jun\"},\n",
    " \"pdays\": {\"0\": -1, \"1\": -1},\n",
    " \"poutcome\": {\"0\": \"unknown\", \"1\": \"unknown\"},\n",
    " \"previous\": {\"0\": 0, \"1\": 0}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will begin to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frontload all of the library imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# set global state variable\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start I will not include reading the csv into a dataframe and grabbing the numeric and categorical columns as this varies depending on the dataset. I would expect someone who is working with this model to do this manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to create functions to wrap in pipeline\n",
    "\n",
    "class PipelineHelper:\n",
    "    \n",
    "        \n",
    "    # generate full dataframe from csv, numeric and categorical column names\n",
    "    def generate_df_cols(csv_path, target):\n",
    "        \n",
    "        # read the csv into a dataframe\n",
    "        df = pd.read_csv(csv_path, delimiter = ';')\n",
    "\n",
    "        # separate numeric, categorical and target columns\n",
    "        num_columns = df.select_dtypes(['integer', 'float']).columns\n",
    "        cat_columns = df.select_dtypes(['object']).drop(columns = target).columns\n",
    "        \n",
    "        # print column types\n",
    "        print(\"Numeric columns are {}.\".format(\", \".join(num_cols)))\n",
    "        print(\"Categorical columns are {}.\".format(\", \".join(cat_cols)))\n",
    "        \n",
    "        return df, cat_columns, num_columns\n",
    "\n",
    "    # split the data with a 90/10 training vs test\n",
    "    def split_data(df, target):\n",
    "        # split the data into training and testing data as 90-10 split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df.drop(columns = target), df[target], \n",
    "                                                        test_size = 0.10, random_state = 42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    # after training pipeline get the precision and recall score\n",
    "    def get_metrics(y_test, y_pred):\n",
    "       \n",
    "        precision_test = precision_score(y_test, y_pred, pos_label = 'yes') * 100\n",
    "        recall_test = recall_score(y_test, y_pred, pos_label = 'yes') * 100\n",
    "        print(\"Precision = {:.0f}% and recall = {:.0f}% on the validation data.\".format(precision_test, recall_test))\n",
    "        \n",
    "        return precision_test, recall_test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a python submodule with these functions that can be utilized for future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns are age, balance, day, duration, campaign, pdays, previous.\n",
      "Categorical columns are job, marital, education, default, housing, loan, contact, month, poutcome.\n"
     ]
    }
   ],
   "source": [
    "# generate data needed pre pipeline\n",
    "pipe_help = PipelineHelper\n",
    "\n",
    "csv_path = \"./bank-full.csv\"\n",
    "\n",
    "df, cat_cols, num_cols = pipe_help.generate_df_cols(csv_path=csv_path, target='y')\n",
    "\n",
    "# get training and testing data\n",
    "X_train, X_test, y_train, y_test = pipe_help.split_data(df, target='y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frontload all of the library imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# set global state variable\n",
    "SEED = 42\n",
    "\n",
    "# # read the csv into a dataframe\n",
    "# bank = pd.read_csv('./bank-full.csv', delimiter = ';')\n",
    "\n",
    "# # separate numeric, categorical and target columns\n",
    "# num_columns = bank.select_dtypes(['integer', 'float']).columns\n",
    "# cat_columns = bank.select_dtypes(['object']).drop(columns = \"y\").columns\n",
    "\n",
    "# # print column types\n",
    "# print(\"Numeric columns are {}.\".format(\", \".join(num_cols)))\n",
    "# print(\"Categorical columns are {}.\".format(\", \".join(cat_cols)))\n",
    "\n",
    "# # split the data into training and testing data as 90-10 split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(bank.drop(columns = \"y\"), bank[\"y\"], \n",
    "#                                                     test_size = 0.10, random_state = 42)\n",
    "\n",
    "# create pre processing functions\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocess\", \n",
    "        ColumnTransformer([ # preprocess data at beginning of pipeline\n",
    "            (\"onehot\", OneHotEncoder(sparse_output=False), cat_cols),\n",
    "            (\"standardize\", StandardScaler(), num_cols)\n",
    "        ])\n",
    "    ),\n",
    "    (\"logit\", LogisticRegression(max_iter = 5000, solver = 'lbfgs', random_state=SEED))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocess&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;onehot&#x27;,\n",
       "                                                  OneHotEncoder(sparse_output=False),\n",
       "                                                  Index([&#x27;job&#x27;, &#x27;marital&#x27;, &#x27;education&#x27;, &#x27;default&#x27;, &#x27;housing&#x27;, &#x27;loan&#x27;, &#x27;contact&#x27;,\n",
       "       &#x27;month&#x27;, &#x27;poutcome&#x27;],\n",
       "      dtype=&#x27;object&#x27;)),\n",
       "                                                 (&#x27;standardize&#x27;,\n",
       "                                                  StandardScaler(),\n",
       "                                                  Index([&#x27;age&#x27;, &#x27;balance&#x27;, &#x27;day&#x27;, &#x27;duration&#x27;, &#x27;campaign&#x27;, &#x27;pdays&#x27;, &#x27;previous&#x27;], dtype=&#x27;object&#x27;))])),\n",
       "                (&#x27;logit&#x27;, LogisticRegression(max_iter=5000, random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocess&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;onehot&#x27;,\n",
       "                                                  OneHotEncoder(sparse_output=False),\n",
       "                                                  Index([&#x27;job&#x27;, &#x27;marital&#x27;, &#x27;education&#x27;, &#x27;default&#x27;, &#x27;housing&#x27;, &#x27;loan&#x27;, &#x27;contact&#x27;,\n",
       "       &#x27;month&#x27;, &#x27;poutcome&#x27;],\n",
       "      dtype=&#x27;object&#x27;)),\n",
       "                                                 (&#x27;standardize&#x27;,\n",
       "                                                  StandardScaler(),\n",
       "                                                  Index([&#x27;age&#x27;, &#x27;balance&#x27;, &#x27;day&#x27;, &#x27;duration&#x27;, &#x27;campaign&#x27;, &#x27;pdays&#x27;, &#x27;previous&#x27;], dtype=&#x27;object&#x27;))])),\n",
       "                (&#x27;logit&#x27;, LogisticRegression(max_iter=5000, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocess: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;onehot&#x27;, OneHotEncoder(sparse_output=False),\n",
       "                                 Index([&#x27;job&#x27;, &#x27;marital&#x27;, &#x27;education&#x27;, &#x27;default&#x27;, &#x27;housing&#x27;, &#x27;loan&#x27;, &#x27;contact&#x27;,\n",
       "       &#x27;month&#x27;, &#x27;poutcome&#x27;],\n",
       "      dtype=&#x27;object&#x27;)),\n",
       "                                (&#x27;standardize&#x27;, StandardScaler(),\n",
       "                                 Index([&#x27;age&#x27;, &#x27;balance&#x27;, &#x27;day&#x27;, &#x27;duration&#x27;, &#x27;campaign&#x27;, &#x27;pdays&#x27;, &#x27;previous&#x27;], dtype=&#x27;object&#x27;))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">onehot</label><div class=\"sk-toggleable__content\"><pre>Index([&#x27;job&#x27;, &#x27;marital&#x27;, &#x27;education&#x27;, &#x27;default&#x27;, &#x27;housing&#x27;, &#x27;loan&#x27;, &#x27;contact&#x27;,\n",
       "       &#x27;month&#x27;, &#x27;poutcome&#x27;],\n",
       "      dtype=&#x27;object&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(sparse_output=False)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">standardize</label><div class=\"sk-toggleable__content\"><pre>Index([&#x27;age&#x27;, &#x27;balance&#x27;, &#x27;day&#x27;, &#x27;duration&#x27;, &#x27;campaign&#x27;, &#x27;pdays&#x27;, &#x27;previous&#x27;], dtype=&#x27;object&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=5000, random_state=42)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocess',\n",
       "                 ColumnTransformer(transformers=[('onehot',\n",
       "                                                  OneHotEncoder(sparse_output=False),\n",
       "                                                  Index(['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact',\n",
       "       'month', 'poutcome'],\n",
       "      dtype='object')),\n",
       "                                                 ('standardize',\n",
       "                                                  StandardScaler(),\n",
       "                                                  Index(['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous'], dtype='object'))])),\n",
       "                ('logit', LogisticRegression(max_iter=5000, random_state=42))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try to use this on X_train data\n",
    "X_train_processed = pipeline.fit(X_train, y_train)\n",
    "X_train_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/logit_pipeline.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save pipeline\n",
    "from joblib import dump, load\n",
    "import json\n",
    "\n",
    "dump(pipeline, './data/logit_pipeline.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for X_test\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 63.0 | Recall: 34.0\n"
     ]
    }
   ],
   "source": [
    "# get precision and recall\n",
    "# Show precision and recall for both training and testing predictions\n",
    "precision_test = precision_score(y_test, y_pred, pos_label = 'yes') * 100\n",
    "recall_test = recall_score(y_test, y_pred, pos_label = 'yes') * 100\n",
    "\n",
    "print(f\"Precision: {precision_test.round(0)}\", \"|\",f\"Recall: {recall_test.round(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logit = dump(logit_regressor, 'filename.joblib') \n",
    "# load json file\n",
    "# load the pipeline\n",
    "new_data_path = \"./data/new_data.json\"\n",
    "pipeline_path = \"./data/logit_pipeline.joblib\"\n",
    "\n",
    "def load_df(json_path):\n",
    "    with open(json_path, \"r\") as j:\n",
    "        data = json.load(j)\n",
    "    new_df = pd.DataFrame(data)\n",
    "    return new_df\n",
    "\n",
    "logit_pipeline = load(pipeline_path)\n",
    "new_df = load_df(new_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>campaign</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>default</th>\n",
       "      <th>duration</th>\n",
       "      <th>education</th>\n",
       "      <th>housing</th>\n",
       "      <th>job</th>\n",
       "      <th>loan</th>\n",
       "      <th>marital</th>\n",
       "      <th>month</th>\n",
       "      <th>pdays</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>previous</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>580</td>\n",
       "      <td>1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>16</td>\n",
       "      <td>no</td>\n",
       "      <td>192</td>\n",
       "      <td>secondary</td>\n",
       "      <td>yes</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>no</td>\n",
       "      <td>married</td>\n",
       "      <td>may</td>\n",
       "      <td>-1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>3644</td>\n",
       "      <td>2</td>\n",
       "      <td>unknown</td>\n",
       "      <td>9</td>\n",
       "      <td>no</td>\n",
       "      <td>83</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>services</td>\n",
       "      <td>no</td>\n",
       "      <td>single</td>\n",
       "      <td>jun</td>\n",
       "      <td>-1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  balance  campaign  contact  day default  duration  education housing  \\\n",
       "0   40      580         1  unknown   16      no       192  secondary     yes   \n",
       "1   47     3644         2  unknown    9      no        83  secondary      no   \n",
       "\n",
       "           job loan  marital month  pdays poutcome  previous y_pred  \n",
       "0  blue-collar   no  married   may     -1  unknown         0     no  \n",
       "1     services   no   single   jun     -1  unknown         0     no  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "new_df['y_pred'] = logit_pipeline.predict(new_df)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save into new json file\n",
    "new_json_path = './data/new_preds.json'\n",
    "new_json = new_df.to_json(new_json_path, orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write dependencies to data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=\"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/conda.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $data/conda.yaml\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.11\n",
    "  - pip\n",
    "  - pip:\n",
    "    - scikit-learn==1.3.0\n",
    "    - pandas==2.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of assignment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
